{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948fdd1f-a2a0-4bc7-a3c1-3096683409e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "data = pl.read_csv('https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e80f8b-00ca-482d-91ce-f1e72a024b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>lead_source</th><th>industry</th><th>number_of_courses_viewed</th><th>annual_income</th><th>employment_status</th><th>location</th><th>interaction_count</th><th>lead_score</th><th>converted</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;paid_ads&quot;</td><td>null</td><td>1</td><td>79450</td><td>&quot;unemployed&quot;</td><td>&quot;south_america&quot;</td><td>4</td><td>0.94</td><td>1</td></tr><tr><td>&quot;social_media&quot;</td><td>&quot;retail&quot;</td><td>1</td><td>46992</td><td>&quot;employed&quot;</td><td>&quot;south_america&quot;</td><td>1</td><td>0.8</td><td>0</td></tr><tr><td>&quot;events&quot;</td><td>&quot;healthcare&quot;</td><td>5</td><td>78796</td><td>&quot;unemployed&quot;</td><td>&quot;australia&quot;</td><td>3</td><td>0.69</td><td>1</td></tr><tr><td>&quot;paid_ads&quot;</td><td>&quot;retail&quot;</td><td>2</td><td>83843</td><td>null</td><td>&quot;australia&quot;</td><td>1</td><td>0.87</td><td>0</td></tr><tr><td>&quot;referral&quot;</td><td>&quot;education&quot;</td><td>3</td><td>85012</td><td>&quot;self_employed&quot;</td><td>&quot;europe&quot;</td><td>3</td><td>0.62</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ lead_sour ┆ industry  ┆ number_of ┆ annual_in ┆ … ┆ location  ┆ interacti ┆ lead_scor ┆ converte │\n",
       "│ ce        ┆ ---       ┆ _courses_ ┆ come      ┆   ┆ ---       ┆ on_count  ┆ e         ┆ d        │\n",
       "│ ---       ┆ str       ┆ viewed    ┆ ---       ┆   ┆ str       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆           ┆ ---       ┆ i64       ┆   ┆           ┆ i64       ┆ f64       ┆ i64      │\n",
       "│           ┆           ┆ i64       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ paid_ads  ┆ null      ┆ 1         ┆ 79450     ┆ … ┆ south_ame ┆ 4         ┆ 0.94      ┆ 1        │\n",
       "│           ┆           ┆           ┆           ┆   ┆ rica      ┆           ┆           ┆          │\n",
       "│ social_me ┆ retail    ┆ 1         ┆ 46992     ┆ … ┆ south_ame ┆ 1         ┆ 0.8       ┆ 0        │\n",
       "│ dia       ┆           ┆           ┆           ┆   ┆ rica      ┆           ┆           ┆          │\n",
       "│ events    ┆ healthcar ┆ 5         ┆ 78796     ┆ … ┆ australia ┆ 3         ┆ 0.69      ┆ 1        │\n",
       "│           ┆ e         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ paid_ads  ┆ retail    ┆ 2         ┆ 83843     ┆ … ┆ australia ┆ 1         ┆ 0.87      ┆ 0        │\n",
       "│ referral  ┆ education ┆ 3         ┆ 85012     ┆ … ┆ europe    ┆ 3         ┆ 0.62      ┆ 1        │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35330578-0ab2-434b-b2db-1c5cc67eac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1462, 9)\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 9)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ lead_sour ┆ industry  ┆ number_of ┆ annual_in ┆ … ┆ location  ┆ interacti ┆ lead_scor ┆ converte │\n",
      "│ ce        ┆ ---       ┆ _courses_ ┆ come      ┆   ┆ ---       ┆ on_count  ┆ e         ┆ d        │\n",
      "│ ---       ┆ str       ┆ viewed    ┆ ---       ┆   ┆ str       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ str       ┆           ┆ ---       ┆ i64       ┆   ┆           ┆ i64       ┆ f64       ┆ i64      │\n",
      "│           ┆           ┆ i64       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ paid_ads  ┆ null      ┆ 1         ┆ 79450     ┆ … ┆ south_ame ┆ 4         ┆ 0.94      ┆ 1        │\n",
      "│           ┆           ┆           ┆           ┆   ┆ rica      ┆           ┆           ┆          │\n",
      "│ social_me ┆ retail    ┆ 1         ┆ 46992     ┆ … ┆ south_ame ┆ 1         ┆ 0.8       ┆ 0        │\n",
      "│ dia       ┆           ┆           ┆           ┆   ┆ rica      ┆           ┆           ┆          │\n",
      "│ events    ┆ healthcar ┆ 5         ┆ 78796     ┆ … ┆ australia ┆ 3         ┆ 0.69      ┆ 1        │\n",
      "│           ┆ e         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ paid_ads  ┆ retail    ┆ 2         ┆ 83843     ┆ … ┆ australia ┆ 1         ┆ 0.87      ┆ 0        │\n",
      "│ referral  ┆ education ┆ 3         ┆ 85012     ┆ … ┆ europe    ┆ 3         ┆ 0.62      ┆ 1        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "Null values per column:\n",
      "shape: (1, 9)\n",
      "┌────────────┬──────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
      "│ lead_sourc ┆ industry ┆ number_of ┆ annual_in ┆ … ┆ location ┆ interacti ┆ lead_scor ┆ converted │\n",
      "│ e          ┆ ---      ┆ _courses_ ┆ come      ┆   ┆ ---      ┆ on_count  ┆ e         ┆ ---       │\n",
      "│ ---        ┆ u32      ┆ viewed    ┆ ---       ┆   ┆ u32      ┆ ---       ┆ ---       ┆ u32       │\n",
      "│ u32        ┆          ┆ ---       ┆ u32       ┆   ┆          ┆ u32       ┆ u32       ┆           │\n",
      "│            ┆          ┆ u32       ┆           ┆   ┆          ┆           ┆           ┆           │\n",
      "╞════════════╪══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 128        ┆ 134      ┆ 0         ┆ 181       ┆ … ┆ 63       ┆ 0         ┆ 0         ┆ 0         │\n",
      "└────────────┴──────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘\n",
      "\n",
      "Columns with missing values: ['lead_source', 'industry', 'annual_income', 'employment_status', 'location']\n",
      "  lead_source: 128 missing values\n",
      "  industry: 134 missing values\n",
      "  annual_income: 181 missing values\n",
      "  employment_status: 100 missing values\n",
      "  location: 63 missing values\n",
      "\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n",
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Filling 128 missing values in 'lead_source' with 'NA'\n",
      "Filling 134 missing values in 'industry' with 'NA'\n",
      "Filling 181 missing values in 'annual_income' with 0.0\n",
      "Filling 100 missing values in 'employment_status' with 'NA'\n",
      "Filling 63 missing values in 'location' with 'NA'\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Count null values per column\n",
    "null_counts = df.null_count()\n",
    "print(\"\\nNull values per column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Show only columns with missing values\n",
    "columns_with_nulls = [col for col in df.columns if df[col].null_count() > 0]\n",
    "print(f\"\\nColumns with missing values: {columns_with_nulls}\")\n",
    "for col in columns_with_nulls:\n",
    "    print(f\"  {col}: {df[col].null_count()} missing values\")\n",
    "\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = [col for col in df.columns if df[col].dtype in [pl.Int64, pl.Float64, pl.Int32, pl.Float32]]\n",
    "categorical_cols = [col for col in df.columns if df[col].dtype == pl.Utf8]\n",
    "\n",
    "print(f\"\\nNumerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Create expressions for filling missing values\n",
    "fill_expressions = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].null_count() > 0:\n",
    "        if col in categorical_cols:\n",
    "            print(f\"Filling {df[col].null_count()} missing values in '{col}' with 'NA'\")\n",
    "            fill_expressions.append(pl.col(col).fill_null('NA').alias(col))\n",
    "        elif col in numerical_cols:\n",
    "            print(f\"Filling {df[col].null_count()} missing values in '{col}' with 0.0\")\n",
    "            fill_expressions.append(pl.col(col).fill_null(0.0).alias(col))\n",
    "        else:\n",
    "            fill_expressions.append(pl.col(col))\n",
    "    else:\n",
    "        fill_expressions.append(pl.col(col))\n",
    "\n",
    "# Apply the transformations\n",
    "df_prepared = df.select(fill_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10283375-bcdd-4037-a6c0-8f10663055eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>lead_source</th><th>industry</th><th>number_of_courses_viewed</th><th>annual_income</th><th>employment_status</th><th>location</th><th>interaction_count</th><th>lead_score</th><th>converted</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;1462&quot;</td><td>&quot;1462&quot;</td><td>1462.0</td><td>1462.0</td><td>&quot;1462&quot;</td><td>&quot;1462&quot;</td><td>1462.0</td><td>1462.0</td><td>1462.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>2.031464</td><td>52472.172367</td><td>null</td><td>null</td><td>2.976744</td><td>0.506108</td><td>0.619015</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>1.449717</td><td>24254.34703</td><td>null</td><td>null</td><td>1.681564</td><td>0.288465</td><td>0.485795</td></tr><tr><td>&quot;min&quot;</td><td>&quot;NA&quot;</td><td>&quot;NA&quot;</td><td>0.0</td><td>0.0</td><td>&quot;NA&quot;</td><td>&quot;NA&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>1.0</td><td>44084.0</td><td>null</td><td>null</td><td>2.0</td><td>0.26</td><td>0.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>2.0</td><td>57474.0</td><td>null</td><td>null</td><td>3.0</td><td>0.51</td><td>1.0</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>3.0</td><td>68243.0</td><td>null</td><td>null</td><td>4.0</td><td>0.75</td><td>1.0</td></tr><tr><td>&quot;max&quot;</td><td>&quot;social_media&quot;</td><td>&quot;technology&quot;</td><td>9.0</td><td>109899.0</td><td>&quot;unemployed&quot;</td><td>&quot;south_america&quot;</td><td>11.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ lead_sour ┆ industry  ┆ number_of ┆ … ┆ location  ┆ interacti ┆ lead_scor ┆ converte │\n",
       "│ ---       ┆ ce        ┆ ---       ┆ _courses_ ┆   ┆ ---       ┆ on_count  ┆ e         ┆ d        │\n",
       "│ str       ┆ ---       ┆ str       ┆ viewed    ┆   ┆ str       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ str       ┆           ┆ ---       ┆   ┆           ┆ f64       ┆ f64       ┆ f64      │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 1462      ┆ 1462      ┆ 1462.0    ┆ … ┆ 1462      ┆ 1462.0    ┆ 1462.0    ┆ 1462.0   │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 0.0       ┆ … ┆ 0         ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ null      ┆ 2.031464  ┆ … ┆ null      ┆ 2.976744  ┆ 0.506108  ┆ 0.619015 │\n",
       "│ std       ┆ null      ┆ null      ┆ 1.449717  ┆ … ┆ null      ┆ 1.681564  ┆ 0.288465  ┆ 0.485795 │\n",
       "│ min       ┆ NA        ┆ NA        ┆ 0.0       ┆ … ┆ NA        ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ 25%       ┆ null      ┆ null      ┆ 1.0       ┆ … ┆ null      ┆ 2.0       ┆ 0.26      ┆ 0.0      │\n",
       "│ 50%       ┆ null      ┆ null      ┆ 2.0       ┆ … ┆ null      ┆ 3.0       ┆ 0.51      ┆ 1.0      │\n",
       "│ 75%       ┆ null      ┆ null      ┆ 3.0       ┆ … ┆ null      ┆ 4.0       ┆ 0.75      ┆ 1.0      │\n",
       "│ max       ┆ social_me ┆ technolog ┆ 9.0       ┆ … ┆ south_ame ┆ 11.0      ┆ 1.0       ┆ 1.0      │\n",
       "│           ┆ dia       ┆ y         ┆           ┆   ┆ rica      ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2c31ad-a5e1-4e61-9685-7e4d5cf30cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>industry</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;retail&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'industry' [str]\n",
       "[\n",
       "\t\"retail\"\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared['industry'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379ed04d-8721-484d-978c-9c76d6521834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical features for correlation: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n"
     ]
    }
   ],
   "source": [
    "numerical_features = [col for col in numerical_cols if col != 'converted']\n",
    "print(f\"\\nNumerical features for correlation: {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f82bab9-f8ea-48ea-b0dc-f0fb77d3c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = df_prepared.select(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5af309-5ce2-4f57-8126-79d5a673ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = {}\n",
    "for col1 in numerical_features:\n",
    "    correlations[col1] = {}\n",
    "    for col2 in numerical_features:\n",
    "        corr = df_numerical.select(pl.corr(col1, col2)).item()\n",
    "        correlations[col1][col2] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55f9922-44ee-436c-adc5-1832f068be9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number_of_courses_viewed': {'number_of_courses_viewed': 1.0,\n",
       "  'annual_income': 0.009770285756444626,\n",
       "  'interaction_count': -0.023565222882887944,\n",
       "  'lead_score': -0.004878998354681265},\n",
       " 'annual_income': {'number_of_courses_viewed': 0.009770285756444633,\n",
       "  'annual_income': 1.0,\n",
       "  'interaction_count': 0.027036472404814337,\n",
       "  'lead_score': 0.015609546050138964},\n",
       " 'interaction_count': {'number_of_courses_viewed': -0.02356522288288794,\n",
       "  'annual_income': 0.02703647240481433,\n",
       "  'interaction_count': 1.0,\n",
       "  'lead_score': 0.009888182496913107},\n",
       " 'lead_score': {'number_of_courses_viewed': -0.004878998354681265,\n",
       "  'annual_income': 0.01560954605013897,\n",
       "  'interaction_count': 0.009888182496913112,\n",
       "  'lead_score': 1.0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfb6dee-12e0-4020-b551-06278bc2684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_courses_viewed        1.0000  0.0098 -0.0236 -0.0049 \n",
      "annual_income                   0.0098  1.0000  0.0270  0.0156 \n",
      "interaction_count              -0.0236  0.0270  1.0000  0.0099 \n",
      "lead_score                     -0.0049  0.0156  0.0099  1.0000 \n"
     ]
    }
   ],
   "source": [
    "for col in numerical_features:\n",
    "    print(f\"{col:30s}\", end=\" \")\n",
    "    for col2 in numerical_features:\n",
    "        print(f\"{correlations[col][col2]:7.4f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9871253-116e-4995-83bc-bffa315320fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_count <-> lead_score: 0.0099\n",
      "number_of_courses_viewed <-> lead_score: -0.0049\n",
      "number_of_courses_viewed <-> interaction_count: -0.0236\n",
      "annual_income <-> interaction_count: 0.0270\n",
      "   annual_income and interaction_count: 0.0270\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'interaction_count')\n",
    "]\n",
    "\n",
    "pair_correlations = {}\n",
    "for feat1, feat2 in pairs:\n",
    "    corr_value = correlations[feat1][feat2]\n",
    "    pair_correlations[(feat1, feat2)] = corr_value\n",
    "    print(f\"{feat1} <-> {feat2}: {corr_value:.4f}\")\n",
    "\n",
    "# Find the pair with highest correlation\n",
    "max_pair = max(pair_correlations.items(), key=lambda x: x[1])\n",
    "print(f\"   {max_pair[0][0]} and {max_pair[0][1]}: {max_pair[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8b179c-49a7-4aef-a46d-186fa85eaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Splitting the data (60%/20%/20%)\n",
      "============================================================\n",
      "\n",
      "Original dataset: 1462 rows\n",
      "Target distribution:\n",
      "converted\n",
      "1    905\n",
      "0    557\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train set: 877 rows (60.0%)\n",
      "Val set:   292 rows (20.0%)\n",
      "Test set:  293 rows (20.0%)\n",
      "\n",
      "============================================================\n",
      "QUESTION 3: Mutual Information Score\n",
      "============================================================\n",
      "\n",
      "Categorical features: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "\n",
      "Encoding categorical features...\n",
      "employment_status        : 0.02\n",
      "industry                 : 0.02\n",
      "lead_source              : 0.03\n",
      "location                 : 0.00\n",
      "industry                 : 0.02\n",
      "location                 : 0.00\n",
      "lead_source              : 0.03\n",
      "employment_status        : 0.02\n",
      "\n",
      "🎯 ANSWER: The variable with the biggest mutual information score is:\n",
      "   'lead_source' with a score of 0.03\n"
     ]
    }
   ],
   "source": [
    "### Question 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# # ============================================================\n",
    "# # Load and prepare data (from Q1)\n",
    "# # ============================================================\n",
    "# url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv'\n",
    "# df = pl.read_csv(url)\n",
    "\n",
    "# # Fill missing values\n",
    "# numerical_cols = [col for col in df.columns if df[col].dtype in [pl.Int64, pl.Float64, pl.Int32, pl.Float32]]\n",
    "# categorical_cols = [col for col in df.columns if df[col].dtype == pl.Utf8]\n",
    "\n",
    "# fill_expressions = []\n",
    "# for col in df.columns:\n",
    "#     if df[col].null_count() > 0:\n",
    "#         if col in categorical_cols:\n",
    "#             fill_expressions.append(pl.col(col).fill_null('NA').alias(col))\n",
    "#         elif col in numerical_cols:\n",
    "#             fill_expressions.append(pl.col(col).fill_null(0.0).alias(col))\n",
    "#         else:\n",
    "#             fill_expressions.append(pl.col(col))\n",
    "#     else:\n",
    "#         fill_expressions.append(pl.col(col))\n",
    "\n",
    "# df_prepared = df.select(fill_expressions)\n",
    "\n",
    "# print(\"Data prepared!\")\n",
    "# print(f\"Shape: {df_prepared.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# Split the data (from Q2)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Splitting the data (60%/20%/20%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to pandas for sklearn (we'll use numpy arrays)\n",
    "df_pandas = df_prepared.to_pandas()\n",
    "\n",
    "# Separate features and target\n",
    "X = df_pandas.drop('converted', axis=1)\n",
    "y = df_pandas['converted']\n",
    "\n",
    "print(f\"\\nOriginal dataset: {len(X)} rows\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# First split: 60% train, 40% temp (which will be split into val and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: split the 40% into 20% val and 20% test (50-50 split of the 40%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} rows ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Val set:   {len(X_val)} rows ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:  {len(X_test)} rows ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "categorical_features = [col for col in categorical_cols if col != 'converted']\n",
    "print(f\"\\nCategorical features: {categorical_features}\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "X_train_encoded = X_train.copy()\n",
    "\n",
    "encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "X_train_categorical = X_train_encoded[categorical_features]\n",
    "\n",
    "mi_scores = mutual_info_classif(\n",
    "    X_train_categorical, \n",
    "    y_train, \n",
    "    discrete_features=True,  # All features are categorical (encoded)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mi_dict = {}\n",
    "for feature, score in zip(categorical_features, mi_scores):\n",
    "    rounded_score = round(score, 2)\n",
    "    mi_dict[feature] = rounded_score\n",
    "\n",
    "for feature in sorted(mi_dict.keys()):\n",
    "    print(f\"{feature:25s}: {mi_dict[feature]:.2f}\")\n",
    "\n",
    "question_vars = ['industry', 'location', 'lead_source', 'employment_status']\n",
    "question_scores = {var: mi_dict[var] for var in question_vars if var in mi_dict}\n",
    "\n",
    "for var in question_vars:\n",
    "    print(f\"{var:25s}: {question_scores[var]:.2f}\")\n",
    "\n",
    "# Find the variable with the biggest MI score\n",
    "max_var = max(question_scores.items(), key=lambda x: x[1])\n",
    "print(f\"\\n🎯 ANSWER: The variable with the biggest mutual information score is:\")\n",
    "print(f\"   '{max_var[0]}' with a score of {max_var[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3456732-a947-405f-a574-0e6c0fc3255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 877 rows\n",
      "Val set:   292 rows\n",
      "Test set:  293 rows\n",
      "\n",
      "Categorical features: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical features: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
      "Original categorical features: 4\n",
      "After one-hot encoding: 27 features\n",
      "Numerical features: 4\n",
      "Final feature count: 31\n",
      "  - Numerical: 4\n",
      "  - One-hot encoded categorical: 27\n",
      "Fitting model on training data...\n",
      "Validation Accuracy: 0.7432\n",
      "Rounded to 2 decimals: 0.74\n",
      "ANSWER: The validation accuracy is: 0.74\n",
      "Training set accuracy: 0.7469\n",
      "Validation set accuracy: 0.7432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_pandas = df_prepared.to_pandas()\n",
    "\n",
    "# Separate features and target\n",
    "X = df_pandas.drop('converted', axis=1)\n",
    "y = df_pandas['converted']\n",
    "\n",
    "# Split: 60% train, 40% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp: 20% val, 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} rows\")\n",
    "print(f\"Val set:   {len(X_val)} rows\")\n",
    "print(f\"Test set:  {len(X_test)} rows\")\n",
    "\n",
    "\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = [col for col in categorical_cols if col != 'converted']\n",
    "numerical_features = [col for col in numerical_cols if col != 'converted']\n",
    "\n",
    "print(f\"\\nCategorical features: {categorical_features}\")\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "\n",
    "# Create OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_cat_encoded = ohe.fit_transform(X_train[categorical_features])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_features])\n",
    "X_test_cat_encoded = ohe.transform(X_test[categorical_features])\n",
    "\n",
    "print(f\"Original categorical features: {len(categorical_features)}\")\n",
    "print(f\"After one-hot encoding: {X_train_cat_encoded.shape[1]} features\")\n",
    "\n",
    "X_train_num = X_train[numerical_features].values\n",
    "X_val_num = X_val[numerical_features].values\n",
    "X_test_num = X_test[numerical_features].values\n",
    "\n",
    "print(f\"Numerical features: {X_train_num.shape[1]}\")\n",
    "\n",
    "X_train_final = np.concatenate([X_train_num, X_train_cat_encoded], axis=1)\n",
    "X_val_final = np.concatenate([X_val_num, X_val_cat_encoded], axis=1)\n",
    "X_test_final = np.concatenate([X_test_num, X_test_cat_encoded], axis=1)\n",
    "\n",
    "print(f\"Final feature count: {X_train_final.shape[1]}\")\n",
    "print(f\"  - Numerical: {X_train_num.shape[1]}\")\n",
    "print(f\"  - One-hot encoded categorical: {X_train_cat_encoded.shape[1]}\")\n",
    "\n",
    "model = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting model on training data...\")\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = model.predict(X_val_final)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_accuracy_rounded = round(val_accuracy, 2)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Rounded to 2 decimals: {val_accuracy_rounded}\")\n",
    "\n",
    "print(f\"ANSWER: The validation accuracy is: {val_accuracy_rounded}\")\n",
    "print(f\"Training set accuracy: {accuracy_score(y_train, model.predict(X_train_final)):.4f}\")\n",
    "print(f\"Validation set accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72031e6a-53bb-44b7-855d-169003137052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All features: ['lead_source', 'industry', 'employment_status', 'location', 'number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
      "Baseline accuracy (all features): 0.743151\n",
      "Without 'lead_source': accuracy = 0.729452, diff = 0.013699\n",
      "Without 'industry': accuracy = 0.743151, diff = 0.000000\n",
      "Without 'employment_status': accuracy = 0.746575, diff = -0.003425\n",
      "Without 'location': accuracy = 0.743151, diff = 0.000000\n",
      "Without 'number_of_courses_viewed': accuracy = 0.678082, diff = 0.065068\n",
      "Without 'annual_income': accuracy = 0.856164, diff = -0.113014\n",
      "Without 'interaction_count': accuracy = 0.674658, diff = 0.068493\n",
      "Without 'lead_score': accuracy = 0.743151, diff = 0.000000\n",
      "\n",
      "Features sorted by importance (difference):\n",
      "annual_income                 : diff = -0.113014\n",
      "employment_status             : diff = -0.003425\n",
      "industry                      : diff = 0.000000\n",
      "location                      : diff = 0.000000\n",
      "lead_score                    : diff = 0.000000\n",
      "lead_source                   : diff = 0.013699\n",
      "number_of_courses_viewed      : diff = 0.065068\n",
      "interaction_count             : diff = 0.068493\n",
      "industry                      : 0.000000\n",
      "employment_status             : -0.003425\n",
      "lead_score                    : 0.000000\n",
      "\n",
      " ANSWER: The feature with the SMALLEST difference is:\n",
      "   'employment_status' with difference = -0.003425\n",
      "\n",
      "This means 'employment_status' is the LEAST USEFUL feature\n",
      "(removing it has the smallest impact on accuracy)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [col for col in categorical_cols if col != 'converted']\n",
    "numerical_features = [col for col in numerical_cols if col != 'converted']\n",
    "all_features = categorical_features + numerical_features\n",
    "\n",
    "print(f\"\\nAll features: {all_features}\")\n",
    "\n",
    "def train_and_evaluate(cat_features, num_features):\n",
    "    \"\"\"Train model with given features and return validation accuracy\"\"\"\n",
    "    \n",
    "    if len(cat_features) > 0:\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_cat = ohe.fit_transform(X_train[cat_features])\n",
    "        X_val_cat = ohe.transform(X_val[cat_features])\n",
    "    else:\n",
    "        X_train_cat = np.array([]).reshape(len(X_train), 0)\n",
    "        X_val_cat = np.array([]).reshape(len(X_val), 0)\n",
    "    \n",
    "    if len(num_features) > 0:\n",
    "        X_train_num = X_train[num_features].values\n",
    "        X_val_num = X_val[num_features].values\n",
    "    else:\n",
    "        X_train_num = np.array([]).reshape(len(X_train), 0)\n",
    "        X_val_num = np.array([]).reshape(len(X_val), 0)\n",
    "    \n",
    "    X_train_final = np.concatenate([X_train_num, X_train_cat], axis=1)\n",
    "    X_val_final = np.concatenate([X_val_num, X_val_cat], axis=1)\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_final, y_train)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_val_pred = model.predict(X_val_final)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "baseline_accuracy = train_and_evaluate(categorical_features, numerical_features)\n",
    "print(f\"Baseline accuracy (all features): {baseline_accuracy:.6f}\")\n",
    "\n",
    "feature_importance = {}\n",
    "\n",
    "for feature in all_features:\n",
    "    # Exclude this feature\n",
    "    if feature in categorical_features:\n",
    "        cat_feats = [f for f in categorical_features if f != feature]\n",
    "        num_feats = numerical_features\n",
    "    else:\n",
    "        cat_feats = categorical_features\n",
    "        num_feats = [f for f in numerical_features if f != feature]\n",
    "    \n",
    "    # Train model without this feature\n",
    "    accuracy_without = train_and_evaluate(cat_feats, num_feats)\n",
    "    \n",
    "    # Calculate difference (how much accuracy dropped)\n",
    "    difference = baseline_accuracy - accuracy_without\n",
    "    feature_importance[feature] = {\n",
    "        'accuracy_without': accuracy_without,\n",
    "        'difference': difference\n",
    "    }\n",
    "    \n",
    "    print(f\"Without '{feature}': accuracy = {accuracy_without:.6f}, diff = {difference:.6f}\")\n",
    "\n",
    "\n",
    "print(\"\\nFeatures sorted by importance (difference):\")\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1]['difference'])\n",
    "\n",
    "for feature, info in sorted_features:\n",
    "    print(f\"{feature:30s}: diff = {info['difference']:.6f}\")\n",
    "\n",
    "\n",
    "question_features = ['industry', 'employment_status', 'lead_score']\n",
    "for feature in question_features:\n",
    "    if feature in feature_importance:\n",
    "        diff = feature_importance[feature]['difference']\n",
    "        print(f\"{feature:30s}: {diff:.6f}\")\n",
    "\n",
    "# Find the feature with smallest difference\n",
    "smallest_diff_feature = min(\n",
    "    [(f, info['difference']) for f, info in feature_importance.items() \n",
    "     if f in question_features],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "print(f\"\\n ANSWER: The feature with the SMALLEST difference is:\")\n",
    "print(f\"   '{smallest_diff_feature[0]}' with difference = {smallest_diff_feature[1]:.6f}\")\n",
    "print(f\"\\nThis means '{smallest_diff_feature[0]}' is the LEAST USEFUL feature\")\n",
    "print(\"(removing it has the smallest impact on accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81a084b6-05af-4577-8e76-b4173c2d0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete!\n",
      "Train: 877, Val: 292, Test: 293\n",
      "\n",
      "Categorical features: 4\n",
      "Numerical features: 4\n",
      "Final feature count: 31\n",
      "\n",
      "Trying C values: [0.01, 0.1, 1, 10, 100]\n",
      "\n",
      "Note: Smaller C = stronger regularization\n",
      "      Larger C = weaker regularization\n",
      "\n",
      "C =   0.01 -> Validation Accuracy = 0.743151 (rounded: 0.743)\n",
      "C =   0.10 -> Validation Accuracy = 0.743151 (rounded: 0.743)\n",
      "C =   1.00 -> Validation Accuracy = 0.743151 (rounded: 0.743)\n",
      "C =  10.00 -> Validation Accuracy = 0.743151 (rounded: 0.743)\n",
      "C = 100.00 -> Validation Accuracy = 0.743151 (rounded: 0.743)\n",
      "\n",
      "C values ranked by validation accuracy:\n",
      "🎯 1. C =   0.01 -> Accuracy = 0.743\n",
      "   2. C =   0.10 -> Accuracy = 0.743\n",
      "   3. C =   1.00 -> Accuracy = 0.743\n",
      "   4. C =  10.00 -> Accuracy = 0.743\n",
      "   5. C = 100.00 -> Accuracy = 0.743\n",
      "\n",
      "🎯 ANSWER: The best C value is: 0.01\n",
      "   Validation accuracy: 0.743\n",
      "\n",
      "------------------------------------------------------------\n",
      "Understanding the results:\n",
      "------------------------------------------------------------\n",
      "C parameter controls regularization strength:\n",
      "  - Small C (e.g., 0.01) = Strong regularization = Simpler model\n",
      "  - Large C (e.g., 100) = Weak regularization = More complex model\n"
     ]
    }
   ],
   "source": [
    "df_pandas = df_prepared.to_pandas()\n",
    "\n",
    "X = df_pandas.drop('converted', axis=1)\n",
    "y = df_pandas['converted']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data split complete!\")\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "categorical_features = [col for col in categorical_cols if col != 'converted']\n",
    "numerical_features = [col for col in numerical_cols if col != 'converted']\n",
    "\n",
    "print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_cat_encoded = ohe.fit_transform(X_train[categorical_features])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_features])\n",
    "\n",
    "# Get numerical features\n",
    "X_train_num = X_train[numerical_features].values\n",
    "X_val_num = X_val[numerical_features].values\n",
    "\n",
    "# Combine features\n",
    "X_train_final = np.concatenate([X_train_num, X_train_cat_encoded], axis=1)\n",
    "X_val_final = np.concatenate([X_val_num, X_val_cat_encoded], axis=1)\n",
    "\n",
    "print(f\"Final feature count: {X_train_final.shape[1]}\")\n",
    "\n",
    "# Define C values to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "print(f\"\\nTrying C values: {C_values}\")\n",
    "print(\"\\nNote: Smaller C = stronger regularization\")\n",
    "print(\"      Larger C = weaker regularization\\n\")\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    # Train model with this C value\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        C=C,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_final, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_final)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracy_rounded = round(accuracy, 3)\n",
    "    \n",
    "    # Store results\n",
    "    results[C] = {\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_rounded': accuracy_rounded\n",
    "    }\n",
    "    \n",
    "    print(f\"C = {C:6.2f} -> Validation Accuracy = {accuracy:.6f} (rounded: {accuracy_rounded})\")\n",
    "\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "\n",
    "print(\"\\nC values ranked by validation accuracy:\")\n",
    "for i, (C, info) in enumerate(sorted_results, 1):\n",
    "    marker = \"🎯\" if i == 1 else \"  \"\n",
    "    print(f\"{marker} {i}. C = {C:6.2f} -> Accuracy = {info['accuracy_rounded']}\")\n",
    "\n",
    "best_C = sorted_results[0][0]\n",
    "best_accuracy = sorted_results[0][1]['accuracy_rounded']\n",
    "\n",
    "print(f\"\\n🎯 ANSWER: The best C value is: {best_C}\")\n",
    "print(f\"   Validation accuracy: {best_accuracy}\")\n",
    "\n",
    "# Additional insight\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Understanding the results:\")\n",
    "print(\"-\"*60)\n",
    "print(\"C parameter controls regularization strength:\")\n",
    "print(\"  - Small C (e.g., 0.01) = Strong regularization = Simpler model\")\n",
    "print(\"  - Large C (e.g., 100) = Weak regularization = More complex model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be635390-b18e-44f1-a267-6a6326cf2c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
